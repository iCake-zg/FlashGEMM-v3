# FlashGEMM-v3
Implement a **general FP16/BF16 GEMM + Softmax fusion operator** on **NVIDIA GPU**, integrate it with **ONNX-Runtime backend** via **TVM Auto-Tuning**, and finally provide **end-to-end accuracy verification report**



## 2 PLAN ğŸ§ 

### Week 1 éœ€æ±‚æ‹†è§£ & ç®—å­åˆ†æ

- **Day1** é€‰æ¨¡å‹ï¼šViT-B/16ï¼ˆCVï¼‰+ Whisper Encoderï¼ˆè¯­éŸ³ï¼‰+ BERT-Largeï¼ˆNLPï¼‰+ DLRMï¼ˆæ¨èï¼‰  
    ç”¨ PyTorch å¯¼å‡º ONNX â†’ Netron æ‰“å¼€ â†’ ç»Ÿè®¡ MatMul+Softmax ç»´åº¦åˆ†å¸ƒã€‚

    - day1 
        - src/getmodel.py  
        - ç”Ÿæˆ models/bert_large.onnx 
        - ç”Ÿæˆ models/whisper_encoder.onnx  
        - æ¨¡å‹æ–‡ä»¶

    
- **Day2** å®šä¹‰ç®—å­æ¥å£ï¼š`fused_gemm_softmax(M,N,K, A,B, bias, mask)`ï¼Œæ”¯æŒ FP16/BF16ã€mask å¯é€‰ã€‚

    - day2
        - æ‰‹å†™è‡ªå®šä¹‰gemmä»¥åŠsoftmaxå‡½æ•°
        - flash_gemm_softmax.py
        - tests/flash_gemm_softmax_test.py
    

    
- **Day3** ç²¾åº¦åŸºçº¿ï¼šONNXRuntime CUDA EP è·‘ FP32 ä½œä¸º goldenã€‚

    - day3 
        - golden_baseline.py
        - å¯¹æ¯”onnxruntimeå’Œpytorchåº•å±‚çš„è®¡ç®—è¯¯å·®
        - results/golden_baseline_results.json ç»“æœ
        
- **Day4** Roofline åˆ†æï¼šNsight Compute â†’ è®¡ç®—ç®—æœ¯å¼ºåº¦ã€å¸¦å®½ä¸Šé™ã€‚
    
- **Day5** éœ€æ±‚å†»ç»“ï¼šè¾“å‡ºã€ŠFlashGEMM-V3 SPECã€‹markdownã€‚
    

### Week 2 CUDA æ ¸å¿ƒç®—å­å¼€å‘

- **Day6-7** æ‰‹å†™ FP16 GEMM tile-based kernelï¼ˆ128Ã—128Ã—32ï¼‰
    
    - ä½¿ç”¨ shared memory double buffer
        
    - warp-level MMA (`mma.sync`)
        
- **Day8** Softmax èåˆï¼šGEMM è¾“å‡º tile ç›´æ¥åœ¨ shared memory åš row-wise softmaxï¼Œé¿å…å†™å› globalã€‚
    
- **Day9** æ”¯æŒ causal maskï¼ˆNLP ç”¨ï¼‰â†’ æ¡ä»¶ load mask å¸¸é‡åˆ° registerã€‚
    
- **Day10** å•å…ƒæµ‹è¯•ï¼špytest + cupy.allclose è¯¯å·® < 1e-3ã€‚
    

### Week 3 Triton ç‰ˆæœ¬ & æ€§èƒ½å¯¹æ¯”

- **Day11-12** ç”¨ Triton DSL é‡å†™åŒä¸€ç®—å­ < 100 è¡Œã€‚
    
- **Day13** Triton Autotuner æœç´¢ tileã€num_stagesã€num_warpsã€‚
    
- **Day14** æ€§èƒ½æ‰“æ“‚å°ï¼šCUDA vs Triton vs cuBLAS+Softmax åˆ†ç¦»ï¼›è®°å½• GFLOPs ä¸å¸¦å®½ã€‚
    

### Week 4 Auto-Tuning & ä½æ¯”ç‰¹å®éªŒ

- **Day15-16** TVM Meta-Scheduleï¼š
    
    - å®šä¹‰ TensorIR scheduleï¼šblockIdx/blockIdx k-split + shared memory cacheã€‚
        
    - è·‘ 1024 random schedules â†’ é€‰ top-10ã€‚
        
- **Day17** BF16 è·¯å¾„ï¼šä¿®æ”¹ PTX `.target sm_89` + `__nv_bfloat16` ç±»å‹ã€‚
    
- **Day18** INT8 é‡åŒ–å°è¯•ï¼šcutlass 3.0 `GemmUniversal` + `FastInterleavedAndBiased` epilogueï¼Œè®°å½•ç²¾åº¦ä¸‹é™ã€‚
    

### Week 5 ç¼–è¯‘é›†æˆ & æ¡†æ¶å¯¹æ¥

- **Day19** ONNXRuntime Custom EPï¼š
    
    - æ–°å»º `flash_gemm_softmax_op.cc` æ³¨å†Œ kernelã€‚
        
    - CMake é“¾æ¥ä½ çš„ `.so`ã€‚
        
- **Day20** TensorFlow pluginï¼šTF-Custom-Op-CC æ¨¡æ¿ï¼Œæ³¨å†Œ `FusedGemmSoftmax`ã€‚
    
- **Day21** ç«¯åˆ°ç«¯æµ‹è¯•ï¼š
    
    - ViT-B æ¨ç† batch=16ï¼ŒéªŒè¯ top-1 è¯¯å·® â‰¤ 0.05 %ã€‚
        
    - Nsight Systems æŸ¥çœ‹ kernel èåˆå global memory traffic â†“ 30%ã€‚
        

### Week 6 ç²¾åº¦éªŒè¯ & é¡¹ç›®æ”¶å®˜

- **Day22** è¯¯å·®åˆ†æè„šæœ¬ï¼š
    
    Python
    
    å¤åˆ¶
    
    ```python
    np.testing.assert_allclose(golden, actual, rtol=1e-3, atol=1e-4)
    ```
    
- **Day23** Corner case æµ‹è¯•ï¼šK=1ã€M=1ã€N=65536ã€å¸¦ dropout maskã€‚
    
- **Day24** æ€§èƒ½å›å½’ï¼šç”¨ AirSpeed Velocity å†™ benchmarkï¼ŒCI è§¦å‘ã€‚
    
- **Day25** æ’°å†™ READMEï¼šæ¶æ„å›¾ + è°ƒä¼˜æ›²çº¿ + ç²¾åº¦è¡¨ã€‚
    
- **Day26** å†…éƒ¨ tech talkï¼ˆ15 minï¼‰ï¼Œå½•å±ä¸Šä¼ ã€‚
    
- **Day27-28** Buffer daysï¼Œå¤„ç† code reviewã€‚
    
- **Day29** Tag v1.0ï¼Œæ‰“ releaseã€‚
    
- **Day30** Retroï¼šåˆ— 3 ä¸ª next stepï¼ˆFlash-Attention3ã€Hopper TMAã€CPython wheelï¼‰ã€‚





## 3 æ–‡ä»¶è¯¦ç»†

- day1 
    getmodel.py  
    ç”Ÿæˆ bert_large.onnx 
    ç”Ÿæˆ whisper_encoder.onnx  
    æ¨¡å‹æ–‡ä»¶


